### 机器学习

------

特征提取-------->标签

散点图(特征)------>决策面

------

##### 监督学习-----朴素贝叶斯(Naive Bayes)：

```
from sklearn.naive_bayes import GaussianNB
clf=GaussianNB()
clf.fit(features_train,labels_train)
pred=clf.predict(features_test)

测试准确率
1.print clf.score(features_test,labels_test)
2.使用pred与labels_test做比较
3.from sklearn.metrics import accuracy_score
  print accuracy_score(pred,labels_test)
```

贝叶斯规则

贝叶斯优缺点

例子：根据邮件内容判断作者

```
//计算运行时间
t0 = time()
< 你的 clf.fit() 代码行 >
print "training time:", round(time()-t0, 3), "s"
```

------

##### 监督学习-----SVM(支持向量机)

Margin：间隔

两个数据集间隔最大化

SVM：正确分类标签作为首要考虑，然后对间隔最大化

```
//线性SVM
from sklearn.svm import SVC
clf=SVC(kernel='linear')
clf.fit(features_train,labels_train)
pred=clf.predict(features_test)

from sklearn.metrics import accuracy_score
acc=accuracy_score(pred,labels_test)
```

非线性解决方法(---->核函数)：

​            非线性转线性，其中转换函数为将数据集分开的线性函数

​             核函数：低维数据转高维

```
//选择SVM的核函数
clf=SVC(kernel='')  
//SVM参数
clf=SVC(c=,gamma=)
```

SVM优缺点

例子：寻找邮件作者

加快速度方法：减少训练数据

------

##### 决策树

```
from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf.fit(features_train,labels_train)
pred=clf.predict(features_test)

from sklearn.metrics import accuracy_score
acc=accuracy_score(pred,labels_test)

//决策树参数
调整参数，提高准确度
```

 熵公式及其计算

决策树优缺点

------

##### 算法选择

k nearest neighbors KNN(K 近邻算法)

random forest(随机森林算法)

adaboost(被提升的决策树)

思路：

1. 了解算法
2. sklearn中查找相关包
3. 使用算法
4. 预测
5. 精度评价

------

##### 数据集与问题

------

##### 回归

年龄：连续

天气：离散

邮件作者：离散

电话号码：离散

收入：连续

连续：有着一定的线性关系

离散：没有次序

```
from sklearn import linear_model
reg=linear_model.LinearRegressiom()
reg.fit(features_train,labels_train)

//回归系数
reg.coef_
//回归截距
reg.intercept_
//R平方
reg.score(features_train,labels_train)

pred=reg.predict(features_test)

from sklearn.metrics import accuracy_score
acc=accuracy_score(pred,labels_test)
```

误差评估

R平方

```
reg.score()
R越大，拟合效果越好
```

线性回归：
$$
y=m*x+b
$$

------

##### 分类与回归

分类：离散    寻找决策边界     准确率

回归：连续     寻找拟合曲线     误差平方和

------

##### 多元回归

------

##### 异常值

1. 训练

2. 去掉误差最大的点

3. 剩下数据训练

4. 重复以上步骤

   ​

------

#### 非监督学习

K-Means算法

1. 分配

2. 优化

   ​

```
//k-means聚类可视化
http://www.naftaliharris.com/blog/visualizing-k-means-clustering/

sklearn.cluster.KMeans(n_clusters=8[聚类点个数],max_iter=300[最大迭代次数],n_init=10[聚类初始化次数])
```

K-means算法依赖于聚类中心点在数据集中的初始位置。

------

##### 特征缩放

若通过合并两个特征从而进行分析的时候，两个特征数值差别较大，从而通过将两个特征转换到[0,1]区间，然后进行特征合并的方法称之为特征缩放。

特征缩放公式：
$$
X'=(X-Xmin)/(Xmax-Xmin)
$$
特征缩放优缺点：

优：数据较为稳定

缺：异常值对特征缩放的影响较大

```
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit_transform()
```

受到特征缩放影响的算法：

使用RBF核函数的SVN                 K-均值聚类

------

##### 文本学习

词袋：每个词出现的频数

```
 from sklearn.feature_extraction.text import CountVectorizer
 vectorizer=CountVectorizer()
 string1=''
 string2=''
 string_list=[string1,string2]
 bag_of_words=vectorizer.fit(string_list)
 bag_of_words=vectorizer.transform(string_list)
 
 vectorizer.vocabulary_get(" ")
```

低信息单词

停止词：比如：[the,is,for,you,will,have,be]

从NLTK中获取停止词

```
from nltk.corpus import stopwords
nltk.download()
sw=stopwords.words("english")
```

词干提取算法

```
from nltk.stem.snowball import SnowballStemmer
stemmer=SnowballStemmer("english")
stemmer.stem(" ")
```

词干提取------>词袋

TFIDF

------

##### 特征选择

添加新特征与删除特征

```
sklearn中单变量特征选择方法：
SelectPercentile：选择最强大的x%特征
SelectKBest：选择K个最强大的特征
```

正则化：处理额外特征

误差+特征------>最小：Lasso Regression:  mininize     SSE+a|贝塔|  其中a为惩罚因子

```
import sklearn.linear_model.Lasso
features,labels=GetMyData()
regression=Lasso()
regression.fit(features,labels)
//预测
regression.predict([2,4])
//参数输出
print regression.coef_
```

------

##### PCA(主成分分析)

数据方差最大化方向

```
from sklearn.decomposition import PCA
pca=PCA(n_components=2)
pca.fit(data)
print pca.explained_variance_ratio
first_pc=pca.components_[0]
second_pc=pca.components_[1]
transformed_data=pca.transform(data)
```

使用情况：

1、访问隐藏特征

2、降维      

​               可视化高维数据

​                降噪    

​                归纳与回归分析前需要进行PCA

------

##### 交叉验证

(train，test)

```
from sklearn import cross_validation
x_train,x_test,y_train,y_test=cross_validation.train_test_split(data,target,test_size=0.4,random_state=0)
```

K折交叉验证

将数据划分为两部分，不会进行打乱随机抽取

```
from sklearn.cross_validation import KFold
kf=KFold(len(data),2)
for train_data,test_data in kf:
    features_train=[xxx[ii] for ii in train_data]
    features_test=[xxx[ii] for ii in test_data]
    labels_train=[***[ii] for ii in train_data]
    labels_test=[***[ii] for ii in test_data]
```

GridSerachCV：

系统遍历多种参数组合，通过交叉验证确定最佳效果参数。

```
parameters={'kernel':('linear','rbf').'C':[1,10]}
svr=svm.SVC()
clf=grid_search.GridSearchCV(svr,paramters)
clf.fit(data,labels)
```

------

##### 总结：

1. 问题与数据
2. 特征选择
3. 算法选择与参数计算
4. 验证