### 数据探索

正太分布判断：

```
import seaborn as sns

sns.displot(data['y'],fit=norm)
备注：添加fit可以显示正太分布曲线
```

若为非正态，可对数据做log变换。

------

变量与目标相关性检验：

```
#单个检验
数值：
var='x1'
data=pd.concat([train['y'],train[var]],axis=1)
data.plot.scatter(x=var,y='y')

标签：
var='x2'
data=pd.connat([train['y'],train[var]],axis=1)
sns.boxplot(x=var,y='y',data=data)

#批量检验
全部检验：
corrmat=data.corr()
sns.heatmap(corrmat)

查看相关性最大的前n个
k=n
cols=corrmat.nlargest(k,'y')['y'].index
cm=np.corrcoef(data[cols].values.T)
sns.set(font_scale=1.25)
sns.heatmap(cm,cbar=True,annot=True,fmt=".2f",annot_kws={'size':10},yticklabels=cols.values,xticklabels=cols.values)

#通过散点图查看相关性
sns.set()
cols=['y','x1','x2','x3','x4','x5']
sns.pairplot(data[cols],size=2.5)
```

缺失值处理

缺失值检查：

```
total=data.isnull().sum().sort_values(ascending=False)
percent=(data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)
missing_data=pd.concat([total,percent],axis=1,key=['Total','Percent'])
```

异常值处理：

```
StandardSacler()  :平均值为0，标准偏差为1
```

文本特征处理

```
#增量编码
from sklearn.preprocessing import LabelEncoder
cols=['x1','x2','x3','x4']
for i in cols:
     lbl=LabelEncoder()
     lbl.fit(list(data[i].values))
     data[i]=lbl.transform(list(data[i].values))

#热门编码
data=pd.get_dummies(data)
```

偏度（skewed）特征：

```
#偏度计算
numeric_feats=data.dtypes[data.dtypes!="object"].index

skewed_feats=data[numeric_feats].apply(lambda x:skew(x.dropna())).sort_values(ascending=False)
skewness=pd.DataFrame({'Skew':skewed_feats})

#特征偏度处理
skewness=skewness[abs(skewness)>0.75]
print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))

from scipy.special import boxcox1p
skewed_features=skewness.index
lam=0.15
for feat in skewed_features:
     data[feat]=boxcox1p(data[feat],lam)
```

模型选择：

用不同数据集对模型进行评估

回归模型：

```
from sklearn.linear_model import ElasticNet,Lasso,BayesianRidge,LassoLarsIC
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.base import BaseEstimator,TransformerMixin,RegressorMixin,clone
from sklearn.model_selection import KFold,cross_val_score,train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb

#验证函数
n_folds=5
def rmsle_cv(model):
    kf=KFold(n_folds,shuffle=True,random_state=42).get_n_splits(train_data_x)
    rmse=np.sqrt(-cross_val_score(model,train_data_x,train_data_y,scoring="neg_mean_squared_error",cv=kf))
    return rmse

#模型建立与评估

#LASSO Regression
lasso=make_pipeline(RobustScaler(),Lasso(alpha=0.0005,random_state=1))

score = rmsle_cv(lasso)
print("Lasso score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

#Elastic Net Regression
ENet=make_pipeline(RobustScaler(),ElasticNet(alpha=0.0005,l1_ratio=.9,random_state=3))

score = rmsle_cv(ENet)
print("ElasticNet score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

#Kernel Ridge Regression
KRR=KernelRidge(alpha=0.6,kernel="polynomial",degree=2,coef0=2.5)

score = rmsle_cv(KRR)
print("Kernel Ridge score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

#Gradient Boosting Regression
GBoost=GradientBoostingRegressor(n_estimators=3000,learning_rate=0.05,max_depth=4,max_features="sqrt",min_samples_leaf=15,min_samples_split=10,loss='huber',random_state=5)

score = rmsle_cv(GBoost)
print("Gradient Boosting score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

#XGBoost
model_xgb=xgb.XGBregressor(colsample_bytree=0.4603,gamma=0.0463,learning_rate=0.05,max_depth=3,min_child_weight=1.7817, n_estimators=2200,reg_alpha=0.4640, reg_lambda=0.8571,subsample=0.5213, silent=1,random_state =7, nthread = -1)

score = rmsle_cv(model_xgb)
print("Xgboost score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

#LightGBM
model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,learning_rate=0.05, n_estimators=720,max_bin = 55, bagging_fraction = 0.8,bagging_freq = 5, feature_fraction = 0.2319,feature_fraction_seed=9, bagging_seed=9,min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)

score = rmsle_cv(model_lgb)
print("LGBM score: {:.4f} ({:.4f})\n" .format(score.mean(), score.std()))
```

模型融合：

```
# Averaged base models class
class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, models):
        self.models = models
        
    # we define clones of the original models to fit the data in
    def fit(self, X, y):
        self.models_ = [clone(x) for x in self.models]
        
        # Train cloned base models
        for model in self.models_:
            model.fit(X, y)

        return self
    
    #Now we do the predictions for cloned models and average them
    def predict(self, X):
        predictions = np.column_stack([
            model.predict(X) for model in self.models_
        ])
        return np.mean(predictions, axis=1) 

averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))

score = rmsle_cv(averaged_models)
print(" Averaged base models score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))



#Adding a Meta-model
class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds
   
    # We again fit the data on clones of the original models
    def fit(self, X, y):
        self.base_models_ = [list() for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)
        
        # Train cloned base models then create out-of-fold predictions
        # that are needed to train the cloned meta-model
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index], y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred
                
        # Now train the cloned  meta-model using the out-of-fold predictions as new feature
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self
   
    #Do the predictions of all base models on the test data and use the averaged predictions as 
    #meta-features for the final prediction which is done by the meta-model
    def predict(self, X):
        meta_features = np.column_stack([
            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
            for base_models in self.base_models_ ])
        return self.meta_model_.predict(meta_features)

stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),meta_model = lasso)
   
score = rmsle_cv(stacked_averaged_models)
print("Stacking Averaged models score: {:.4f} ({:.4f})".format(score.mean(), score.std()))

#平均模型
def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))

stacked_averaged_models.fit(train.values, y_train)
stacked_train_pred = stacked_averaged_models.predict(train.values)
stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))
print(rmsle(y_train, stacked_train_pred))

model_xgb.fit(train, y_train)
xgb_train_pred = model_xgb.predict(train)
xgb_pred = np.expm1(model_xgb.predict(test))
print(rmsle(y_train, xgb_train_pred))

model_lgb.fit(train, y_train)
lgb_train_pred = model_lgb.predict(train)
lgb_pred = np.expm1(model_lgb.predict(test.values))
print(rmsle(y_train, lgb_train_pred))

print('RMSLE score on train data:')
print(rmsle(y_train,stacked_train_pred*0.70 +xgb_train_pred*0.15 + lgb_train_pred*0.15 ))

ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15
           
```
模型融合：

备注：基模型 误差率大于0.5时，模型组合效果会低于基模型

模型融合的方法：

1. 投票

   分类问题：投票多着为最终分类

   改进：加权

2. 求均值

   回归问题：降低过拟合

   改进：加权

3. bagging

   随机森林

4. Boosting

   将弱分类器加权

5. Stacking

------

#### 模型评估：

分类模型：

|        指标        | 描述        | Scikit-learn函数                           |
| :--------------: | --------- | ---------------------------------------- |
|    Precision     | 精准度       | from sklearn.metrics import precision_score |
|      Recall      | 召回率       | from sklearn.metrics import recall_score |
|        F1        | F1值       | from sklearn.metrics import f1_score     |
| Confusion Matrix | 混淆矩阵      | from sklearn.metrics import confusion_matrix |
|       ROC        | ROC曲线     | from sklearn.metrics import roc          |
|       AUC        | ROC曲线下的面积 | from sklearn.metrics import auc          |

备注：若AUC>0.8：分类器效果较好

​            若0.6<AUC<0.8,,，可以通过调参提高分类器性能

​            若AUC<0.6，分析特征

回归模型：

|        指标         | 描述   | Scikit-learn函数                           |
| :---------------: | ---- | ---------------------------------------- |
| Mean Square Error | 平均方差 | from sklearn.metrics import mean_squared_error |
|  Absolute Error   | 绝对误差 | from sklearn.metrics import mean_absolute_error,median_absolute_error |
|     R-Squared     | R平方值 | from sklearn.metrics import r2_score     |

模型调参：

```
以GradientBoostingRegressor为例

model=GradientBoostingRegressor()

param_grid={
            'n_estimators':range(20,80,10),
            'learning_rate':[0.2,0.1,0.05,0.02,0.01],
            'max_depth':[4,6,8],
            'min_simples_leaf':[3,5,9,14],
            'max_features':[0.8,0.3,0.5,0.1]
            ................................
}

from sklearn.model_selection import GridSearchCV
estimator=GridSearchCV(model,param_grid)
estimator.fit(train_x,train_y)

print("得分：",estimator.grid_scores)

print("最优参数：",estimator.best_params_)

print("最优参数得分：",estimator.score(test_x,test_y))
```
特征处理：

相关性分析

对特征进行one-hot编码

强特征组合

特征舍弃

F1值，AUC值，均方根误差

特征交叉    线下cv值

模型融合

模型调参

最重要特征/特征：判断模型重要性



##### 特征工程

特征选择：Pearson系数     基尼系数    信息增益      Random Forest、Gradient boosting做特征选择

特征提取：PCA  ICA   LDA

特征构造

```
RandomForest特征选择：
from sklearn.ensemble import RandomForestRegressor
rfr=RandomForestRegressor()
rfr.fit(data_x,data_y)
print(sorted(zip(map(lambda x: round(x, 18), rfr.feature_importances_),data_x.columns),
             reverse=True))
```
特征选择：

```
删除低方差特征
 from sklearn.feature_selection import VarianceThreshold
 #参数为阈值
 sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
 sel.fit_transform(X)
 
 单变量特征选择
 SelectKBest：K个
 SelectPercentile：百分比个
 
For regression: f_regression, mutual_info_regression
For classification: chi2, f_classif, mutual_info_classif
```

```
数据预处理：
查看数据：
data['x1'].unique()
data['x1'].hist()
data.isnull().sum()

#众数
data,mode()
```
决策树、随机森林不需要对特征进行缩放

| 名称   | 代码              | 说明                               |
| ---- | --------------- | -------------------------------- |
| 归一化  | normalization   | 将特征缩放到（0,1）                      |
| 标准化  | standardization | 对于大部分算法更适用；保留了异常值中的有用信息；近似服从正太分布 |

------

holdout：

考虑将数据划分为三部分：

训练集：训练不同模型

验证集：模型选择

测试集：评估模型泛化能力

缺点：对数据划分结果很敏感

##### K折交叉验证

------

思考：

在评估模型的时候，可以选择K折交叉验证，比较稳定或者直接用k折交叉验证对模型、参数、特征进行判断，以降低k折交叉验证结果的均值为目的进行特征选择与调参。

模型得分与模型误差可以作为评价模型拟合程度的指标

备注：

模型选择、参数评估、特征选择时用k折交叉验证来评估，以减少交叉验证结果和的均值为目标。

数据缩放后交叉验证结果的均值会大幅度减小，但是仍然以减少均值为目标。

------

XGBoost：

理解：将多个弱分类器组合起来。串行训练（后一个模型训练前一个模型无法拟合的数据）

重要的参数：

通用参数：

|                参数名                |                   说明                   |
| :-------------------------------: | :------------------------------------: |
| booster（gbtree, gblinear or dart） | gbtree and dart：树模型      gblinear：线性模型 |

树模型参数：

|          名称           |               说明                |  范围   |
| :-------------------: | :-----------------------------: | :---: |
|   eta[default=0.3]    |           学习率：每次更新的步长           | [0,1] |
|   gamma[default=0]    | min_split_loss：树节点分割所需要的最小损失减少量 | [0,∞] |
| max_depth [default=6] |             树的最大深度              | [0,∞] |

学习参数：

|                    名称                    |
| :--------------------------------------: |
|      objective [default=reg:linear]      |
| eval_metric [default according to objective] |

常用参数经验：

| 参数                                       |    说明     | 一般选取范围     |
| :--------------------------------------- | :-------: | :--------- |
| learning_rate                            | 一般情况下为0.1 | [0.05,0.3] |
| 决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree) |           |            |
| 正则化参数的调优(lambda, alpha)，目的在于降低模型复杂度      |           |            |

| 参数               | 起始值   | 范围                                       |
| ---------------- | ----- | ---------------------------------------- |
| max_depth        | [4,6] | [3,10]                                   |
| min_child_weight | 1     | [1,6]                                    |
| gamma            | 0     | [0]                                      |
| subsample        | 0.8   | [0.6,1]                                  |
| colsample_bytree | 0.8   | [0.6,1]                                  |
| reg_alpha        |       | [1e-5, 1e-2, 0.1, 1, 100]/[0, 0.001, 0.005, 0.01, 0.05] |

备注：重要性依次递减

说明：

treeboost的性能远远好于linear boost，所以linear boost很少使用。

| 参数               | 取值          |
| ---------------- | ----------- |
| eta              | 0.01~0.2    |
| min_child_weight | 过高欠拟合，过低过拟合 |
| max_depth        | 3~10        |
| subsample        | 0.5~1       |
| colsample_bytree | 0.5~1       |